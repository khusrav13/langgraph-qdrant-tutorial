{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì‹¤ìŠµ: 3-ì—ì´ì „íŠ¸ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ\n",
    "\n",
    "## ëª©í‘œ\n",
    "ì´ ì‹¤ìŠµì—ì„œëŠ” 3ê°œì˜ ì—ì´ì „íŠ¸ê°€ í˜‘ë ¥í•˜ëŠ” ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œì„ ë§Œë“­ë‹ˆë‹¤:\n",
    "1. **ê²€ìƒ‰ ì—ì´ì „íŠ¸**: ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë…¼ë¬¸ 5ê°œ ê²€ìƒ‰\n",
    "2. **ìš”ì•½ ì—ì´ì „íŠ¸**: ê²€ìƒ‰ëœ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ê³  ìš”ì•½\n",
    "3. **ì¶”ì²œ ì—ì´ì „íŠ¸**: ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ ë…¼ë¬¸ 3ê°œ ì¶”ì²œ\n",
    "\n",
    "## TODO ê°œìˆ˜: 3ê°œ\n",
    "- TODO 1: 3ê°œì˜ ë„êµ¬ ë§Œë“¤ê¸°\n",
    "- TODO 2: 3ê°œì˜ ì—ì´ì „íŠ¸ í•¨ìˆ˜ ë§Œë“¤ê¸°\n",
    "- TODO 3: LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±\n",
    "\n",
    "**ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ 1ì‹œê°„**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ (ì´ë¯¸ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q qdrant-client transformers torch accelerate sentence-transformers pandas langchain langgraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ ë‹¨ê³„: ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ (ì´ë¯¸ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, VectorParams, PointStruct\n",
    "from langchain.tools import tool\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Any, Optional\n",
    "import operator\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ ë‹¨ê³„: ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬ (ì´ë¯¸ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('arxiv_ai.csv')\n",
    "df = df[['title', 'summary', 'published', 'authors']].dropna()\n",
    "df = df.sample(500, random_state=42).reset_index(drop=True)\n",
    "df['published_year'] = pd.to_datetime(df['published']).dt.year\n",
    "df['text'] = df['title'] + ' ' + df['summary']\n",
    "\n",
    "print(f\"âœ… ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df)}ê°œ ë…¼ë¬¸\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ ë‹¨ê³„: ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print(f\"âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ ë‹¨ê³„: Qdrant ì„¤ì • ë° ë°ì´í„° ì €ì¥ (ì´ë¯¸ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(\":memory:\")\n",
    "collection_name = \"arxiv_papers\"\n",
    "\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config=VectorParams(size=384, distance=Distance.COSINE)\n",
    ")\n",
    "\n",
    "points = []\n",
    "for idx, row in df.iterrows():\n",
    "    vector = embedding_model.encode(row['text']).tolist()\n",
    "    point = PointStruct(\n",
    "        id=idx,\n",
    "        vector=vector,\n",
    "        payload={\n",
    "            \"title\": row['title'],\n",
    "            \"summary\": row['summary'],\n",
    "            \"year\": int(row['published_year']),\n",
    "            \"authors\": row['authors']\n",
    "        }\n",
    "    )\n",
    "    points.append(point)\n",
    "    if (idx + 1) % 100 == 0:\n",
    "        print(f\"{idx + 1}ê°œ ë…¼ë¬¸ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "\n",
    "client.upsert(collection_name=collection_name, points=points)\n",
    "print(f\"\\nâœ… {len(points)}ê°œ ë…¼ë¬¸ì´ ë²¡í„° DBì— ì €ì¥ë¨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì¤€ë¹„ ë‹¨ê³„: Granite ëª¨ë¸ ë¡œë“œ (ì´ë¯¸ ì™„ë£Œ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"ibm-granite/granite-4.0-micro\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device if device == \"cuda\" else None)\n",
    "llm_model.eval()\n",
    "\n",
    "def generate_response(prompt: str, max_tokens: int = 300) -> str:\n",
    "    chat = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat_text = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
    "    input_tokens = tokenizer(chat_text, return_tensors=\"pt\").to(device)\n",
    "    output = llm_model.generate(**input_tokens, max_new_tokens=max_tokens)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=False)\n",
    "    assistant_response = response.split(\"<|start_of_role|>assistant<|end_of_role|>\")[-1]\n",
    "    assistant_response = assistant_response.split(\"<|end_of_text|>\")[0].strip()\n",
    "    return assistant_response\n",
    "\n",
    "print(f\"âœ… Granite ëª¨ë¸ ë¡œë“œ ì™„ë£Œ ({device})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# ì‹¤ìŠµ ì‹œì‘\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 1: 3ê°œì˜ ë„êµ¬(Tools) ë§Œë“¤ê¸° â­\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©í•  3ê°œì˜ ë„êµ¬ë¥¼ ë§Œë“œì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 1-1 (Korean): search_papers_toolì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "# TODO 1-1 (English): Implement search_papers_tool\n",
    "# HINT: @tool ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”\n",
    "# HINT: def search_papers_tool(query: str, top_k: int = 5) -> str:\n",
    "# HINT: client.query_points()ë¡œ ê²€ìƒ‰í•˜ê³  .pointsë¡œ ê²°ê³¼ë¥¼ ë°›ìœ¼ì„¸ìš”\n",
    "# HINT: ê° ë…¼ë¬¸ì˜ ì œëª©, ì—°ë„, ìœ ì‚¬ë„, ìš”ì•½(150ì)ì„ í¬í•¨í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”\n",
    "\n",
    "\n",
    "\n",
    "# TODO 1-2 (Korean): summarize_papers_toolì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "# TODO 1-2 (English): Implement summarize_papers_tool\n",
    "# HINT: def summarize_papers_tool(papers_info: str) -> str:\n",
    "# HINT: generate_response()ë¥¼ ì‚¬ìš©í•˜ì—¬ ë…¼ë¬¸ë“¤ì„ ë¶„ì„í•˜ì„¸ìš”\n",
    "# HINT: í”„ë¡¬í”„íŠ¸ì— \"í•µì‹¬ ê¸°ìˆ \", \"ì ìš© ê°€ëŠ¥ì„±\", \"ì—°ê´€ì„±\"ì„ ë¶„ì„í•˜ë„ë¡ ìš”ì²­í•˜ì„¸ìš”\n",
    "\n",
    "\n",
    "\n",
    "# TODO 1-3 (Korean): recommend_papers_toolì„ êµ¬í˜„í•˜ì„¸ìš”\n",
    "# TODO 1-3 (English): Implement recommend_papers_tool\n",
    "# HINT: def recommend_papers_tool(summary: str, papers_info: str) -> str:\n",
    "# HINT: summaryì™€ papers_infoë¥¼ ëª¨ë‘ ë°›ì•„ì„œ ìµœê³ ì˜ ë…¼ë¬¸ 3ê°œë¥¼ ì¶”ì²œí•˜ì„¸ìš”\n",
    "# HINT: ì¶”ì²œ ì´ìœ , í™œìš© ë°©ë²•, ì½ëŠ” ìˆœì„œë¥¼ í¬í•¨í•˜ë„ë¡ í”„ë¡¬í”„íŠ¸ë¥¼ ì‘ì„±í•˜ì„¸ìš”\n",
    "\n",
    "\n",
    "\n",
    "tools = [search_papers_tool, summarize_papers_tool, recommend_papers_tool]\n",
    "print(\"âœ… TODO 1 ì™„ë£Œ: 3ê°œ ë„êµ¬ ìƒì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 2: 3ê°œì˜ ì—ì´ì „íŠ¸ í•¨ìˆ˜ ë§Œë“¤ê¸° â­\n",
    "\n",
    "ê° ì—ì´ì „íŠ¸ëŠ” ì´ì „ ì—ì´ì „íŠ¸ì˜ ê²°ê³¼ë¥¼ ë°›ì•„ì„œ ì²˜ë¦¬í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    search_results: str\n",
    "    summary: str\n",
    "    recommendations: str\n",
    "    next_step: str\n",
    "\n",
    "# TODO 2-1 (Korean): search_agent í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”\n",
    "# TODO 2-1 (English): Implement search_agent function\n",
    "# HINT: def search_agent(state: AgentState) -> AgentState:\n",
    "# HINT: state[\"query\"]ë¥¼ ì‚¬ìš©í•˜ì—¬ search_papers_toolì„ í˜¸ì¶œí•˜ì„¸ìš”\n",
    "# HINT: ê²°ê³¼ë¥¼ state[\"search_results\"]ì— ì €ì¥í•˜ì„¸ìš”\n",
    "# HINT: state[\"next_step\"] = \"summarize\"ë¡œ ì„¤ì •í•˜ì„¸ìš”\n",
    "\n",
    "\n",
    "\n",
    "# TODO 2-2 (Korean): summarize_agent í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”\n",
    "# TODO 2-2 (English): Implement summarize_agent function\n",
    "# HINT: def summarize_agent(state: AgentState) -> AgentState:\n",
    "# HINT: state[\"search_results\"]ë¥¼ ì‚¬ìš©í•˜ì—¬ summarize_papers_toolì„ í˜¸ì¶œí•˜ì„¸ìš”\n",
    "# HINT: ê²°ê³¼ë¥¼ state[\"summary\"]ì— ì €ì¥í•˜ì„¸ìš”\n",
    "# HINT: state[\"next_step\"] = \"recommend\"ë¡œ ì„¤ì •í•˜ì„¸ìš”\n",
    "\n",
    "\n",
    "\n",
    "# TODO 2-3 (Korean): recommend_agent í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”\n",
    "# TODO 2-3 (English): Implement recommend_agent function\n",
    "# HINT: def recommend_agent(state: AgentState) -> AgentState:\n",
    "# HINT: state[\"summary\"]ì™€ state[\"search_results\"]ë¥¼ ëª¨ë‘ recommend_papers_toolì— ì „ë‹¬í•˜ì„¸ìš”\n",
    "# HINT: ê²°ê³¼ë¥¼ state[\"recommendations\"]ì— ì €ì¥í•˜ì„¸ìš”\n",
    "# HINT: state[\"next_step\"] = \"end\"ë¡œ ì„¤ì •í•˜ì„¸ìš”\n",
    "\n",
    "\n",
    "\n",
    "def router(state: AgentState) -> str:\n",
    "    return state[\"next_step\"]\n",
    "\n",
    "print(\"âœ… TODO 2 ì™„ë£Œ: 3ê°œ ì—ì´ì „íŠ¸ ìƒì„±\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO 3: LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„± â­\n",
    "\n",
    "3ê°œ ì—ì´ì „íŠ¸ë¥¼ ì—°ê²°í•˜ëŠ” ì›Œí¬í”Œë¡œìš°ë¥¼ ë§Œë“œì„¸ìš”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 3 (Korean): LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì„±í•˜ì„¸ìš”\n",
    "# TODO 3 (English): Configure LangGraph workflow\n",
    "# HINT: workflow = StateGraph(AgentState)\n",
    "# HINT: workflow.add_node()ë¡œ 3ê°œ ì—ì´ì „íŠ¸ ì¶”ê°€ (\"search\", \"summarize\", \"recommend\")\n",
    "# HINT: workflow.set_entry_point(\"search\")ë¡œ ì‹œì‘ì  ì„¤ì •\n",
    "# HINT: workflow.add_conditional_edges()ë¡œ ì¡°ê±´ë¶€ ì—£ì§€ ì¶”ê°€\n",
    "#       - \"search\" â†’ router â†’ {\"summarize\": \"summarize\", \"end\": END}\n",
    "#       - \"summarize\" â†’ router â†’ {\"recommend\": \"recommend\", \"end\": END}\n",
    "# HINT: workflow.add_edge(\"recommend\", END)ë¡œ ë§ˆì§€ë§‰ ì—£ì§€ ì¶”ê°€\n",
    "# HINT: app = workflow.compile()\n",
    "\n",
    "\n",
    "\n",
    "print(\"âœ… TODO 3 ì™„ë£Œ: ì›Œí¬í”Œë¡œìš° êµ¬ì„± ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‹œìŠ¤í…œ ì‹¤í–‰ í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_recommendation_system(user_query: str):\n",
    "    initial_state = {\n",
    "        \"query\": user_query,\n",
    "        \"search_results\": \"\",\n",
    "        \"summary\": \"\",\n",
    "        \"recommendations\": \"\",\n",
    "        \"next_step\": \"search\"\n",
    "    }\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(f\"ğŸ“ ì‚¬ìš©ì ì§ˆë¬¸: {user_query}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    result = app.invoke(initial_state)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ” [1ë‹¨ê³„: ê²€ìƒ‰ ê²°ê³¼]\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"search_results\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"ğŸ“Š [2ë‹¨ê³„: ë…¼ë¬¸ ë¶„ì„]\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"summary\"])\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"â­ [3ë‹¨ê³„: ìµœì¢… ì¶”ì²œ Top 3]\")\n",
    "    print(\"=\" * 70)\n",
    "    print(result[\"recommendations\"])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ŒìŠ¤íŠ¸ 1: ì»´í“¨í„° ë¹„ì „"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_recommendation_system(\n",
    "    \"I need papers about deep learning for computer vision in autonomous driving\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í…ŒìŠ¤íŠ¸ 2: ìì—°ì–´ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = run_recommendation_system(\n",
    "    \"Natural language processing for chatbot development with transformers\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê²°ë¡ \n",
    "\n",
    "ì¶•í•˜í•©ë‹ˆë‹¤! 3-ì—ì´ì „íŠ¸ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œì„ ì™„ì„±í–ˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "### í•™ìŠµí•œ ë‚´ìš©:\n",
    "1. âœ… LangChain ë„êµ¬ ë§Œë“¤ê¸° (@tool)\n",
    "2. âœ… ìƒíƒœ ê¸°ë°˜ ì—ì´ì „íŠ¸ êµ¬í˜„\n",
    "3. âœ… LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±\n",
    "4. âœ… ì—ì´ì „íŠ¸ ê°„ ë°ì´í„° ì „ë‹¬\n",
    "\n",
    "### ì‹œìŠ¤í…œ íë¦„:\n",
    "```\n",
    "ì‚¬ìš©ì ì§ˆë¬¸ \n",
    "    â†“\n",
    "ê²€ìƒ‰ ì—ì´ì „íŠ¸ (5ê°œ ë…¼ë¬¸ ê²€ìƒ‰)\n",
    "    â†“\n",
    "ìš”ì•½ ì—ì´ì „íŠ¸ (ë…¼ë¬¸ ë¶„ì„)\n",
    "    â†“\n",
    "ì¶”ì²œ ì—ì´ì „íŠ¸ (Top 3 ì¶”ì²œ)\n",
    "    â†“\n",
    "ìµœì¢… ê²°ê³¼\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
